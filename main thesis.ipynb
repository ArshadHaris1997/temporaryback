{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d93cb420-f6f1-4b16-82c1-d66636530fe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['match_outcome_model_retrained.pkl']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Efficiently load only the necessary columns\n",
    "df = pd.read_csv(\"ODI_Match_Data.csv\", usecols=[\n",
    "    'match_id', 'innings', 'ball', 'runs_off_bat', 'extras', 'player_dismissed'\n",
    "])\n",
    "\n",
    "# Filter second innings\n",
    "df = df[df['innings'] == 2]\n",
    "\n",
    "# Save as a smaller file\n",
    "df.to_csv(\"second_innings_filtered.csv\", index=False)\n",
    "\n",
    "################################################################################\n",
    "\n",
    "# Assuming your filtered DataFrame is named `df`\n",
    "df['total_runs'] = df['runs_off_bat'] + df['extras']\n",
    "df['over'] = df['ball'].astype(str).str.extract(r'^(\\d+)\\.').astype(float)\n",
    "\n",
    "# Group by match and estimate target score\n",
    "df['target_score'] = df.groupby('match_id')['total_runs'].transform('sum') + 1\n",
    "df['current_score'] = df.groupby('match_id')['total_runs'].cumsum()\n",
    "df['is_wicket'] = df['player_dismissed'].notna().astype(int)\n",
    "df['wickets_lost'] = df.groupby('match_id')['is_wicket'].cumsum()\n",
    "df['wickets_in_hand'] = 10 - df['wickets_lost']\n",
    "df['overs_remaining'] = 50 - df['over']\n",
    "df['run_rate'] = df['current_score'] / (50 - df['overs_remaining'] + 0.1)\n",
    "df['required_run_rate'] = (df['target_score'] - df['current_score']) / df['overs_remaining'].replace(0, 1)\n",
    "df['match_pressure_index'] = df['required_run_rate'] / (df['run_rate'] + 0.1)\n",
    "\n",
    "# Match phase\n",
    "df['match_phase'] = pd.cut(df['over'], bins=[0, 10, 40, 50], labels=['early', 'middle', 'death'])\n",
    "df = pd.get_dummies(df, columns=['match_phase'])\n",
    "\n",
    "# Get the final ball of each second-innings match\n",
    "final_ball_df = df.groupby('match_id').tail(1).copy()\n",
    "final_ball_df['match_result'] = (final_ball_df['current_score'] >= final_ball_df['target_score']).astype(int)\n",
    "final_ball_df['toss_winner'] = 0\n",
    "final_ball_df['venue_advantage'] = 0\n",
    "\n",
    "\n",
    "features = [\n",
    "    'target_score', 'current_score', 'wickets_in_hand', 'overs_remaining',\n",
    "    'required_run_rate', 'toss_winner', 'venue_advantage', 'match_pressure_index',\n",
    "    'match_phase_early', 'match_phase_middle', 'match_phase_death'\n",
    "]\n",
    "\n",
    "#######################################################################################\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "X = final_ball_df[features].dropna()\n",
    "y = final_ball_df.loc[X.index, 'match_result']\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "import joblib\n",
    "joblib.dump(model, \"match_outcome_model_retrained.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9159d80d-37dc-40a9-9758-ab8f1a7f4239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "048c8893-2d67-41b2-a06c-7871134a2984",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e624ba5-2806-49f9-b950-9e434821e682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a3c8abce-d708-4f7b-829c-29b163529200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"ODI_Match_Data.csv\", low_memory=False)\n",
    "\n",
    "# Step 1: Total runs per ball\n",
    "df['total_runs'] = df['runs_off_bat'] + df['extras']\n",
    "\n",
    "# Step 2: Extract over number\n",
    "df['over'] = df['ball'].astype(str).str.extract(r'^(\\d+)\\.').astype(float)\n",
    "\n",
    "# Step 3: Identify first and second innings\n",
    "first_innings = df[df['innings'] == 1].groupby('match_id')['total_runs'].sum().reset_index()\n",
    "first_innings.columns = ['match_id', 'target_score']\n",
    "\n",
    "# Step 4: Focus on second innings only\n",
    "second_df = df[df['innings'] == 2].copy()\n",
    "\n",
    "# Merge target score into second innings\n",
    "second_df = pd.merge(second_df, first_innings, on='match_id', how='left')\n",
    "\n",
    "# Step 5: Cumulative features\n",
    "second_df['current_score'] = second_df.groupby('match_id')['total_runs'].cumsum()\n",
    "second_df['is_wicket'] = second_df['player_dismissed'].notna().astype(int)\n",
    "second_df['wickets_lost'] = second_df.groupby('match_id')['is_wicket'].cumsum()\n",
    "second_df['wickets_in_hand'] = 10 - second_df['wickets_lost']\n",
    "second_df['overs_remaining'] = 50 - second_df['over']\n",
    "second_df['required_run_rate'] = (second_df['target_score'] - second_df['current_score']) / second_df['overs_remaining'].replace(0, np.nan)\n",
    "\n",
    "# Step 6: Match Phase\n",
    "second_df['match_phase'] = pd.cut(second_df['over'], bins=[0, 10, 40, 50], labels=['early_phase', 'middle_phase', 'death_phase'])\n",
    "second_df = pd.get_dummies(second_df, columns=['match_phase'])\n",
    "\n",
    "# Step 7: Match Pressure Index\n",
    "second_df['run_rate'] = second_df['current_score'] / (50 - second_df['overs_remaining'] + 0.1)\n",
    "second_df['match_pressure_index'] = second_df['required_run_rate'] / (second_df['run_rate'] + 0.1)\n",
    "\n",
    "# Optional placeholders for features not directly in data\n",
    "second_df['toss_winner'] = 0  # Placeholder, unless calculated\n",
    "second_df['venue_advantage'] = 0  # Placeholder, unless mapped from venue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "98544021-8a3c-4999-888b-9c90d82320bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'target_score', 'current_score', 'wickets_in_hand',\n",
    "    'overs_remaining', 'required_run_rate',\n",
    "    'toss_winner', 'venue_advantage', 'match_pressure_index',\n",
    "    'match_phase_early_phase', 'match_phase_middle_phase', 'match_phase_death_phase'\n",
    "]\n",
    "\n",
    "# For label\n",
    "second_df['final_score'] = second_df.groupby('match_id')['current_score'].transform('max')\n",
    "second_df['match_result'] = (second_df['current_score'] >= second_df['target_score']).astype(int)\n",
    "\n",
    "# Drop missing or divide-by-zero errors\n",
    "model_data = second_df.dropna(subset=features + ['match_result'])\n",
    "\n",
    "X = model_data[features]\n",
    "y = model_data['match_result']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "92be0cc7-5d23-4b4d-a466-cceae53de169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    115043\n",
      "           1       1.00      1.00      1.00       467\n",
      "\n",
      "    accuracy                           1.00    115510\n",
      "   macro avg       1.00      1.00      1.00    115510\n",
      "weighted avg       1.00      1.00      1.00    115510\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model Accuracy:\", model.score(X_test, y_test))\n",
    "print(classification_report(y_test, model.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696ff8aa-3129-4cf7-b84a-4ae12c8fe617",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7978c1dc-8e8f-4754-bcce-41941f9e2fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match_result\n",
      "0    2316\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(final_ball_df['match_result'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c069ba8c-e479-41dc-afd8-63def8a4126b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddafb732-3b1a-41f9-8f42-2bcc121ebb66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbe49a6-95a3-42c2-86e5-bc384939dc9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5428c9dd-a1bb-44c1-b174-babae4e551a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d60b035b-175a-44f2-8181-16802de3e60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8753f0-c8aa-4bd8-bbdd-d088fae1b273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7882d13-94a7-45dd-a325-6cbbe48e2d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3f8398-ae6e-4a3d-9dbb-51ffa3c0d61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "48cb8f84-d9f9-4f79-b770-ea92300fa66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Pivot the data with innings as columns\n",
    "pivot = innings_scores.pivot(index='match_id', columns='innings', values='total_runs')\n",
    "\n",
    "# Step 2: Keep only matches that have exactly 2 innings (1st and 2nd)\n",
    "pivot = pivot[[1, 2]].dropna()\n",
    "pivot.columns = ['innings_1', 'innings_2']\n",
    "\n",
    "# Step 3: Derive match result\n",
    "pivot['match_result'] = (pivot['innings_2'] > pivot['innings_1']).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dd829efd-20ee-4667-945e-f919803efef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.merge(pivot[['match_result']], left_on='match_id', right_index=True, how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ac33121-40a3-4598-bc54-2c85de6c0e67",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m final_df\n",
      "\u001b[1;31mNameError\u001b[0m: name 'final_df' is not defined"
     ]
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dcfef38d-d630-4a91-ab54-fed73dd9fb0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['target_score', 'toss_winner', 'venue_advantage'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X \u001b[38;5;241m=\u001b[39m final_df[features]\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[0;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m final_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatch_result\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['target_score', 'toss_winner', 'venue_advantage'] not in index\""
     ]
    }
   ],
   "source": [
    "X = final_df[features].dropna()\n",
    "y = final_df['match_result']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a99f8a33-c830-4c2e-8f48-222e2b464eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'current_score', 'wickets_in_hand', 'overs_remaining',\n",
    "    'required_run_rate', 'match_pressure_index',\n",
    "    'match_phase_early', 'match_phase_middle', 'match_phase_death'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "046b868e-6ed3-4241-b903-959f8cd5ba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_df[features].dropna()\n",
    "y = final_df['match_result']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "916e1a81-6c78-41cd-9894-df316caa6add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['match_outcome_model_retrained.pkl']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "joblib.dump(model, \"match_outcome_model_retrained.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ac3801-5eb7-48ab-8bfe-e3235e4de608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cef97b-6a95-48d1-abbd-05c1e8addc77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858d2bc0-3d92-46e0-a00c-ed7a4f60de17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac93ce8-4fc7-4f3e-add4-3e90fd6a15b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4931a485-8146-4335-b3fe-56559122e770",
   "metadata": {},
   "source": [
    "Death_Over_Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bf90295f-a90a-4a25-b708-7207c8c8e727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         bowlerName  balls_bowled  dot_balls  runs_conceded  extras  wickets  \\\n",
      "21     Chris Jordan            37         14             52       2       10   \n",
      "8    Arshdeep Singh            54         22             62       6        9   \n",
      "73    Naveen-ul-Haq            44         16             58       4        7   \n",
      "80      Pat Cummins            43         13             47       1        7   \n",
      "84   Rishad Hossain            30         16             34       0        6   \n",
      "48    Kagiso Rabada            57         24             59       3        6   \n",
      "6     Andre Russell            32         12             55       1        6   \n",
      "103     Trent Boult            24         15             10       0        5   \n",
      "100    Taskin Ahmed            31         17             28       1        5   \n",
      "40   Jasprit Bumrah            46         24             29       0        5   \n",
      "\n",
      "     dot_percent    economy  discipline_ratio  \n",
      "21     37.837838   8.432432         94.594595  \n",
      "8      40.740741   6.888889         88.888889  \n",
      "73     36.363636   7.909091         90.909091  \n",
      "80     30.232558   6.558140         97.674419  \n",
      "84     53.333333   6.800000        100.000000  \n",
      "48     42.105263   6.210526         94.736842  \n",
      "6      37.500000  10.312500         96.875000  \n",
      "103    62.500000   2.500000        100.000000  \n",
      "100    54.838710   5.419355         96.774194  \n",
      "40     52.173913   3.782609        100.000000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your Excel file\n",
    "df = pd.read_excel(\"T20_WC_24_All_Matches_Dataset.xlsx\")\n",
    "\n",
    "# Filter for death overs (16–20)\n",
    "death_overs_df = df[(df['over'] >= 16.0) & (df['over'] <= 20.0)].copy()\n",
    "\n",
    "# Prepare metrics\n",
    "death_overs_df['bowlerName'] = death_overs_df['bowlerName'].fillna('Unknown')\n",
    "death_overs_df['isDotBall'] = (death_overs_df['runs'] == 0).astype(int)\n",
    "death_overs_df['isExtra'] = death_overs_df['isWide'] | death_overs_df['isNoBall']\n",
    "death_overs_df['isWicket'] = death_overs_df['isWicket'].astype(bool).astype(int)\n",
    "\n",
    "# Aggregate performance\n",
    "bowler_stats = death_overs_df.groupby('bowlerName').agg(\n",
    "    balls_bowled=('over', 'count'),\n",
    "    dot_balls=('isDotBall', 'sum'),\n",
    "    runs_conceded=('bowlerRuns', 'sum'),\n",
    "    extras=('isExtra', 'sum'),\n",
    "    wickets=('isWicket', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculated fields\n",
    "bowler_stats['dot_percent'] = (bowler_stats['dot_balls'] / bowler_stats['balls_bowled']) * 100\n",
    "bowler_stats['economy'] = bowler_stats['runs_conceded'] / (bowler_stats['balls_bowled'] / 6)\n",
    "bowler_stats['discipline_ratio'] = (1 - (bowler_stats['extras'] / bowler_stats['balls_bowled'])) * 100\n",
    "\n",
    "# Top performers\n",
    "top_bowlers = bowler_stats.sort_values(by=['wickets', 'dot_percent'], ascending=False).head(10)\n",
    "print(top_bowlers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ff0010-6ae7-4be9-89c2-a1e395c6dc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f0a973f7-8637-44d5-afc3-316f4d78e32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your T20 World Cup dataset\n",
    "df = pd.read_excel(\"T20_WC_24_All_Matches_Dataset.xlsx\")\n",
    "\n",
    "# Filter death overs (16 to 20)\n",
    "death_overs_df = df[(df['over'] >= 16.0) & (df['over'] <= 20.0)].copy()\n",
    "\n",
    "# Preprocess\n",
    "death_overs_df['bowlerName'] = death_overs_df['bowlerName'].fillna('Unknown')\n",
    "death_overs_df['isDotBall'] = (death_overs_df['runs'] == 0).astype(int)\n",
    "death_overs_df['isExtra'] = death_overs_df['isWide'] | death_overs_df['isNoBall']\n",
    "death_overs_df['isWicket'] = death_overs_df['isWicket'].astype(bool).astype(int)\n",
    "\n",
    "# Group by bowler\n",
    "bowler_stats = death_overs_df.groupby('bowlerName').agg(\n",
    "    balls_bowled=('over', 'count'),\n",
    "    dot_balls=('isDotBall', 'sum'),\n",
    "    runs_conceded=('bowlerRuns', 'sum'),\n",
    "    extras=('isExtra', 'sum'),\n",
    "    wickets=('isWicket', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate stats\n",
    "bowler_stats['dot_percent'] = (bowler_stats['dot_balls'] / bowler_stats['balls_bowled']) * 100\n",
    "bowler_stats['economy'] = bowler_stats['runs_conceded'] / (bowler_stats['balls_bowled'] / 6)\n",
    "bowler_stats['discipline_ratio'] = (1 - (bowler_stats['extras'] / bowler_stats['balls_bowled'])) * 100\n",
    "\n",
    "# Save to CSV\n",
    "bowler_stats.to_csv(\"death_over_bowler_stats.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f681657-0f1c-4335-ae06-a8feb722abd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fbbe3f-2c0d-4f97-af95-a84deafec2bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f3c44a-5bb5-4645-a902-00c13ba81dce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e7091c65-e89f-4dd7-9ae3-809d55734dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeltaGenerator()"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "\n",
    "st.set_page_config(page_title=\"Death Over Bowler Recommender\", page_icon=\"🏏\")\n",
    "st.title(\"🏏 Best Death Over Bowler Recommender\")\n",
    "\n",
    "st.markdown(\"\"\"\n",
    "Select a pressure level and get recommended bowlers based on wickets, dot ball %, economy, and discipline (death overs only).\n",
    "\"\"\")\n",
    "\n",
    "# 📁 Load pre-computed stats (create this CSV from your notebook)\n",
    "try:\n",
    "    bowler_stats = pd.read_csv(\"death_over_bowler_stats.csv\")\n",
    "except FileNotFoundError:\n",
    "    st.error(\"Please make sure 'death_over_bowler_stats.csv' exists in the same folder.\")\n",
    "    st.stop()\n",
    "\n",
    "# 🔍 Scenario Selection\n",
    "scenario = st.selectbox(\"Select Pressure Scenario\", [\"Low\", \"Medium\", \"High\"])\n",
    "\n",
    "# 🎯 Sort logic based on scenario\n",
    "if scenario == \"Low\":\n",
    "    result = bowler_stats.sort_values(by=[\"discipline_ratio\", \"dot_percent\"], ascending=False)\n",
    "elif scenario == \"Medium\":\n",
    "    result = bowler_stats.sort_values(by=[\"wickets\", \"dot_percent\"], ascending=False)\n",
    "else:  # High pressure\n",
    "    result = bowler_stats.sort_values(by=[\"wickets\", \"discipline_ratio\"], ascending=False)\n",
    "\n",
    "# 📊 Display top 5\n",
    "st.subheader(\"Top Recommended Bowlers\")\n",
    "st.dataframe(result.head(5)[[\"bowlerName\", \"wickets\", \"dot_percent\", \"economy\", \"discipline_ratio\"]])\n",
    "\n",
    "st.caption(\"🔍 Based on overs 16–20 only (death overs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bbddc26f-3587-4070-8924-5a5dc6322a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_excel(\"T20_WC_24_All_Matches_Dataset.xlsx\")\n",
    "\n",
    "# Fill missing names\n",
    "df['batsmanName'] = df['batsmanName'].fillna(\"Unknown\")\n",
    "\n",
    "# Batting stats\n",
    "batsman_stats = df.groupby('batsmanName').agg(\n",
    "    matches=('matchID', 'nunique'),\n",
    "    innings=('inningID', 'nunique'),\n",
    "    total_runs=('batsmanRuns', 'sum'),\n",
    "    balls_faced=('batsmanBall', 'sum'),\n",
    "    boundaries=('isBoundary', 'sum'),\n",
    ").reset_index()\n",
    "\n",
    "# Metrics\n",
    "batsman_stats = batsman_stats[batsman_stats['balls_faced'] > 0]\n",
    "batsman_stats['batting_avg'] = batsman_stats['total_runs'] / batsman_stats['innings']\n",
    "batsman_stats['strike_rate'] = (batsman_stats['total_runs'] / batsman_stats['balls_faced']) * 100\n",
    "batsman_stats['boundary_rate'] = batsman_stats['boundaries'] / batsman_stats['balls_faced'] * 100\n",
    "\n",
    "# Normalize\n",
    "for col in ['batting_avg', 'strike_rate', 'boundary_rate']:\n",
    "    batsman_stats[f'{col}_z'] = (batsman_stats[col] - batsman_stats[col].mean()) / batsman_stats[col].std()\n",
    "\n",
    "# Save\n",
    "batsman_stats.to_csv(\"batsman_performance_stats.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ac179f-3641-45f9-a80c-81035deae309",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa34f655-cdac-49eb-81e9-36ff9677034d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ff66c7-fab3-49fc-afa0-7caef7599cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1ac36ad5-64bb-4b6b-9c4a-97e68c149e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Bowler stats saved to 'bowler_performance_stats.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the match dataset\n",
    "df = pd.read_excel(\"T20_WC_24_All_Matches_Dataset.xlsx\")\n",
    "\n",
    "# 🔧 Preprocessing\n",
    "df['bowlerName'] = df['bowlerName'].fillna('Unknown')\n",
    "df['isWicket'] = df['isWicket'].astype(int)\n",
    "df['isDotBall'] = (df['runs'] == 0).astype(int)\n",
    "df['isExtra'] = df['isWide'].fillna(0) + df['isNoBall'].fillna(0)\n",
    "\n",
    "# 🧮 Aggregated stats per bowler\n",
    "bowler_stats = df.groupby('bowlerName').agg(\n",
    "    matches=('matchID', 'nunique'),\n",
    "    balls_bowled=('bowlerRuns', 'count'),\n",
    "    runs_conceded=('bowlerRuns', 'sum'),\n",
    "    wickets=('isWicket', 'sum'),\n",
    "    dot_balls=('isDotBall', 'sum'),\n",
    "    extras=('isExtra', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# 🔍 Filter bowlers with actual deliveries\n",
    "bowler_stats = bowler_stats[bowler_stats['balls_bowled'] > 0]\n",
    "\n",
    "# 📊 Derived metrics\n",
    "bowler_stats['economy'] = bowler_stats['runs_conceded'] / (bowler_stats['balls_bowled'] / 6)\n",
    "bowler_stats['strike_rate'] = bowler_stats['balls_bowled'] / bowler_stats['wickets'].replace(0, 1)\n",
    "bowler_stats['dot_percent'] = (bowler_stats['dot_balls'] / bowler_stats['balls_bowled']) * 100\n",
    "bowler_stats['discipline_ratio'] = 100 - ((bowler_stats['extras'] / bowler_stats['balls_bowled']) * 100)\n",
    "bowler_stats['wickets_per_match'] = bowler_stats['wickets'] / bowler_stats['matches']\n",
    "\n",
    "# 🧮 Z-score Normalization (for radar plots, clustering, ML)\n",
    "for col in ['economy', 'strike_rate', 'dot_percent', 'discipline_ratio', 'wickets_per_match']:\n",
    "    z_col = f\"{col}_z\"\n",
    "    bowler_stats[z_col] = (bowler_stats[col] - bowler_stats[col].mean()) / bowler_stats[col].std()\n",
    "\n",
    "# 💾 Save the file\n",
    "bowler_stats.to_csv(\"bowler_performance_stats.csv\", index=False)\n",
    "print(\"✅ Bowler stats saved to 'bowler_performance_stats.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "19eae225-21c1-4ce6-9bd5-71d1decd9ad6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'batting_team'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'batting_team'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT20_WC_24_All_Matches_Dataset.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Preprocessing\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatting_team\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatting_team\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mover\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mover\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[0;32m      9\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mruns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mruns\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'batting_team'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your Excel dataset\n",
    "df = pd.read_excel(\"T20_WC_24_All_Matches_Dataset.xlsx\")\n",
    "\n",
    "# Preprocessing\n",
    "df['batting_team'] = df['batting_team'].fillna('Unknown')\n",
    "df['over'] = df['over'].astype(float)\n",
    "df['runs'] = df['runs'].fillna(0)\n",
    "df['isWicket'] = df['isWicket'].astype(int)\n",
    "\n",
    "# Phase categorization\n",
    "def assign_phase(over):\n",
    "    if over < 6:\n",
    "        return 'Powerplay'\n",
    "    elif over < 16:\n",
    "        return 'Middle'\n",
    "    else:\n",
    "        return 'Death'\n",
    "\n",
    "df['phase'] = df['over'].apply(assign_phase)\n",
    "\n",
    "# Aggregate by team and phase\n",
    "team_phase_stats = df.groupby(['batting_team', 'phase']).agg(\n",
    "    runs_total=('runs', 'sum'),\n",
    "    balls_faced=('over', 'count'),\n",
    "    wickets_lost=('isWicket', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate run rate\n",
    "team_phase_stats['run_rate'] = team_phase_stats['runs_total'] / (team_phase_stats['balls_faced'] / 6)\n",
    "\n",
    "# Pivot table for visual use\n",
    "phase_pivot = team_phase_stats.pivot(index='batting_team', columns='phase', values='run_rate').fillna(0).reset_index()\n",
    "\n",
    "# Save for Streamlit or Power BI\n",
    "phase_pivot.to_csv(\"team_phase_efficiency.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "445fb8c6-203e-415f-943c-6cc132c2541b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['matchID', 'matchNo', 'match', 'currentInning', 'inningID', 'over', 'runningOver', 'runs', 'batsmanRuns', 'batsmanBall', 'bowlerRuns', 'shortText', 'batsmanPlayerID', 'batsmanName', 'bowlerPlayerID', 'bowlerName', 'isBoundary', 'isWide', 'isNoBall', 'isLegBye', 'isBye', 'isWicket', 'isBowlerWicket', 'wicketText', 'wktBatsmanName', 'wktBowlerName', 'wktBatsmanRuns', 'wktbatsmanBalls', 'commentary']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"T20_WC_24_All_Matches_Dataset.xlsx\")\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8ace7a6a-0218-4e7c-ad87-c7cff7577510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved to phase_efficiency_batsman_proxy.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_excel(\"T20_WC_24_All_Matches_Dataset.xlsx\")\n",
    "\n",
    "# Convert types and clean\n",
    "df['over'] = df['over'].astype(float)\n",
    "df['runs'] = df['runs'].fillna(0)\n",
    "df['isWicket'] = df['isWicket'].astype(int)\n",
    "df['batsmanName'] = df['batsmanName'].fillna(\"Unknown\")\n",
    "\n",
    "# Phase classification\n",
    "def assign_phase(over):\n",
    "    if over < 6:\n",
    "        return 'Powerplay'\n",
    "    elif over < 16:\n",
    "        return 'Middle'\n",
    "    else:\n",
    "        return 'Death'\n",
    "\n",
    "df['phase'] = df['over'].apply(assign_phase)\n",
    "\n",
    "# Aggregate phase stats per match and batsman (proxy team)\n",
    "phase_stats = df.groupby(['matchID', 'batsmanName', 'phase']).agg(\n",
    "    runs_total=('runs', 'sum'),\n",
    "    balls_faced=('over', 'count'),\n",
    "    wickets_lost=('isWicket', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "phase_stats['run_rate'] = phase_stats['runs_total'] / (phase_stats['balls_faced'] / 6)\n",
    "\n",
    "# Pivot to get phase-wise view per batsman\n",
    "pivot = phase_stats.pivot_table(index='batsmanName', columns='phase', values='run_rate', aggfunc='mean').fillna(0).reset_index()\n",
    "\n",
    "# Save to CSV\n",
    "pivot.to_csv(\"phase_efficiency_batsman_proxy.csv\", index=False)\n",
    "print(\"✅ Saved to phase_efficiency_batsman_proxy.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ca0a92-60f7-497d-9fbd-f281b5f61afb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b31b2f-48b8-42da-913f-914f3ce1b988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256354f2-1b6f-4a69-9d3b-1bfe18ef8817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36ca727-2b0d-4f48-b3de-4bab41402c01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0552c1-1840-4aaf-a068-77fe6501b61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Streamlit Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a3ed6e-906f-47ea-b910-8bb79033f262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a77b8b-5bc5-46e6-8691-345d9f4120cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9c920acf-98bf-4221-be3c-099e8bc6d11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.97      0.71       258\n",
      "           1       0.70      0.09      0.16       218\n",
      "\n",
      "    accuracy                           0.57       476\n",
      "   macro avg       0.63      0.53      0.43       476\n",
      "weighted avg       0.62      0.57      0.45       476\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "# Load ODI Match Data\n",
    "df = pd.read_csv(\"ODI_Match_Data.csv\", low_memory=False)\n",
    "\n",
    "# Basic cleaning\n",
    "df['venue'] = df['venue'].fillna(\"Unknown\")\n",
    "df['batting_team'] = df['batting_team'].fillna(\"Unknown\")\n",
    "df['bowling_team'] = df['bowling_team'].fillna(\"Unknown\")\n",
    "\n",
    "# For simulation, we’ll assume:\n",
    "# Toss winner = batting_team in 1st innings\n",
    "df['toss_decision'] = df['innings'].apply(lambda x: 'bat' if x == 1 else 'bowl')\n",
    "\n",
    "# Aggregate outcome per match\n",
    "df['total_runs'] = df['runs_off_bat'] + df['extras']\n",
    "match_scores = df.groupby(['match_id', 'innings'])['total_runs'].sum().unstack()\n",
    "match_scores['match_result'] = (match_scores[2] > match_scores[1]).astype(int)\n",
    "\n",
    "# Merge with toss and venue info\n",
    "meta = df[['match_id', 'venue', 'batting_team', 'innings']].drop_duplicates()\n",
    "toss_info = meta[meta['innings'] == 1].copy()\n",
    "toss_info = toss_info.rename(columns={'batting_team': 'toss_winner'})\n",
    "toss_info['toss_decision'] = 'bat'\n",
    "toss_info = toss_info[['match_id', 'venue', 'toss_winner', 'toss_decision']]\n",
    "toss_info = toss_info.merge(match_scores[['match_result']], left_on='match_id', right_index=True)\n",
    "\n",
    "# Encode categorical variables\n",
    "X = pd.get_dummies(toss_info[['venue', 'toss_winner', 'toss_decision']], drop_first=True)\n",
    "y = toss_info['match_result']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Decision Tree\n",
    "model = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model, \"toss_decision_model.pkl\")\n",
    "\n",
    "# Report\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test, model.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d5bc4654-0eea-47f6-b2b6-81910afd1ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2968960065.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  input_encoded[col] = 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeltaGenerator()"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Load model and options\n",
    "model = joblib.load(\"toss_decision_model.pkl\")\n",
    "venues = ['Lord\\'s', 'MCG', 'Eden Gardens', 'Wankhede', 'Old Trafford']  # Sample venues\n",
    "teams = ['India', 'Australia', 'England', 'Pakistan', 'South Africa']\n",
    "\n",
    "# App config\n",
    "st.set_page_config(page_title=\"Captaincy Toss Decision Simulator\", page_icon=\"🧢\")\n",
    "st.title(\"🧢 Toss Decision Recommender\")\n",
    "st.markdown(\"Simulate the better toss decision based on historical match outcomes.\")\n",
    "\n",
    "# Inputs\n",
    "venue = st.selectbox(\"🏟️ Select Venue\", venues)\n",
    "toss_winner = st.selectbox(\"🏏 Toss Winner\", teams)\n",
    "toss_decision = st.radio(\"🧭 Toss Decision\", ['bat', 'bowl'])\n",
    "\n",
    "# Create input DataFrame for prediction\n",
    "input_df = pd.DataFrame({\n",
    "    'venue': [venue],\n",
    "    'toss_winner': [toss_winner],\n",
    "    'toss_decision': [toss_decision]\n",
    "})\n",
    "input_encoded = pd.get_dummies(input_df)\n",
    "model_features = model.feature_names_in_\n",
    "missing_cols = set(model_features) - set(input_encoded.columns)\n",
    "for col in missing_cols:\n",
    "    input_encoded[col] = 0\n",
    "input_encoded = input_encoded[model_features]\n",
    "\n",
    "# Predict\n",
    "prediction = model.predict(input_encoded)[0]\n",
    "result_label = \"👍 Win Likely\" if prediction == 1 else \"👎 Loss Likely\"\n",
    "st.subheader(f\"🎯 Prediction: {result_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88786c4-c9b7-4452-b99c-6ffa11371164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e91baf79-3329-4b79-97dd-2b296ce387ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.27      0.38       220\n",
      "           1       0.53      0.84      0.65       217\n",
      "\n",
      "    accuracy                           0.55       437\n",
      "   macro avg       0.58      0.56      0.51       437\n",
      "weighted avg       0.58      0.55      0.51       437\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "# 🔹 Load match data\n",
    "df = pd.read_csv(\"ODI_Match_Data.csv\", low_memory=False)\n",
    "\n",
    "# 🔹 Basic cleanup\n",
    "df['venue'] = df['venue'].fillna(\"Unknown\")\n",
    "df['batting_team'] = df['batting_team'].fillna(\"Unknown\")\n",
    "df['bowling_team'] = df['bowling_team'].fillna(\"Unknown\")\n",
    "\n",
    "# 🔹 Assume toss_winner = batting_team in 1st innings\n",
    "df['toss_decision'] = df['innings'].apply(lambda x: 'bat' if x == 1 else 'bowl')\n",
    "df['total_runs'] = df['runs_off_bat'] + df['extras']\n",
    "\n",
    "# ✅ STEP 1: Total runs per match and innings\n",
    "innings_runs = df.groupby(['match_id', 'innings'])['total_runs'].sum().reset_index()\n",
    "\n",
    "# ✅ STEP 2: Filter out matches beyond 2 innings (e.g. Super Over, Tests)\n",
    "innings_runs = innings_runs[innings_runs['innings'].isin([1, 2])]\n",
    "\n",
    "# ✅ STEP 3: Pivot to wide format\n",
    "innings_pivot = innings_runs.pivot(index='match_id', columns='innings', values='total_runs')\n",
    "innings_pivot = innings_pivot.dropna()  # Keep only matches with both innings\n",
    "\n",
    "# ✅ STEP 4: Rename columns and compute result\n",
    "innings_pivot.columns = ['1st_innings', '2nd_innings']\n",
    "innings_pivot['match_result'] = (innings_pivot['2nd_innings'] > innings_pivot['1st_innings']).astype(int)\n",
    "\n",
    "# ✅ STEP 5: Extract toss metadata (from 1st innings perspective)\n",
    "meta = df[['match_id', 'venue', 'batting_team', 'bowling_team', 'innings']].drop_duplicates()\n",
    "toss_meta = meta[meta['innings'] == 1].copy()\n",
    "toss_meta['toss_winner'] = toss_meta['batting_team']\n",
    "toss_meta['toss_decision'] = 'bat'  # Because they batted first\n",
    "\n",
    "# ✅ STEP 6: Join metadata with match result\n",
    "toss_df = toss_meta.merge(innings_pivot[['match_result']], left_on='match_id', right_index=True)\n",
    "\n",
    "# ✅ STEP 7: Handle class imbalance\n",
    "df_win = toss_df[toss_df['match_result'] == 1]\n",
    "df_loss = toss_df[toss_df['match_result'] == 0]\n",
    "\n",
    "df_loss_up = resample(df_loss, replace=True, n_samples=len(df_win), random_state=42)\n",
    "df_balanced = pd.concat([df_win, df_loss_up])\n",
    "\n",
    "# ✅ STEP 8: Encode categorical features\n",
    "X = pd.get_dummies(df_balanced[['venue', 'toss_winner', 'bowling_team', 'toss_decision']], drop_first=True)\n",
    "y = df_balanced['match_result']\n",
    "\n",
    "# ✅ STEP 9: Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ✅ STEP 10: Train Decision Tree model\n",
    "model = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ✅ STEP 11: Save the model\n",
    "joblib.dump(model, \"toss_decision_model_v2.pkl\")\n",
    "\n",
    "# ✅ STEP 12: Evaluate performance\n",
    "print(\"📊 Classification Report:\")\n",
    "print(classification_report(y_test, model.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cdbcd242-f073-44ca-a22c-40f25b7d67d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matchup matrix saved to 'bowler_vs_batsman_matchups.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_excel(\"T20_WC_24_All_Matches_Dataset.xlsx\")\n",
    "\n",
    "# Clean and derive basic info\n",
    "df['runs'] = df['runs'].fillna(0)\n",
    "df['isWicket'] = df['isWicket'].astype(int)\n",
    "df['balls'] = 1  # Every row is one ball\n",
    "\n",
    "# Group by Bowler vs Batsman\n",
    "matchups = df.groupby(['bowlerName', 'batsmanName']).agg(\n",
    "    total_runs=('runs', 'sum'),\n",
    "    balls_faced=('balls', 'sum'),\n",
    "    dismissals=('isWicket', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate Strike Rate\n",
    "matchups['strike_rate'] = (matchups['total_runs'] / matchups['balls_faced']) * 100\n",
    "matchups['dismissal_ratio'] = matchups['dismissals'] / matchups['balls_faced']\n",
    "\n",
    "# Optional: Filter low-ball encounters\n",
    "matchups = matchups[matchups['balls_faced'] >= 6]  # At least 1 over bowled\n",
    "\n",
    "# Save to CSV\n",
    "matchups.to_csv(\"bowler_vs_batsman_matchups.csv\", index=False)\n",
    "print(\"✅ Matchup matrix saved to 'bowler_vs_batsman_matchups.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb015307-341c-4a79-918b-dd6e72d9ab47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8591a1d9-7856-4a2e-99ec-9713f68ce2da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18afa279-a151-4354-87a2-b639928a1a2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911c8a02-a72d-4399-92d6-ce5667514b26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c338f7f-0482-46e7-be0d-481d95d09cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WIN PROBABILITY#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11643a3d-385b-4566-8200-2a71e4810977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9c9b66-8836-4985-ae62-2536d795d68d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cb2445-7056-4108-9339-e00ea5940155",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic regression#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9f564cad-53e3-48cf-901e-980e1690623a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model trained and saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# Load ball-by-ball data\n",
    "df = pd.read_csv(\"ODI_Match_Data.csv\", low_memory=False)\n",
    "\n",
    "# Total runs per ball\n",
    "df['total_runs'] = df['runs_off_bat'] + df['extras']\n",
    "\n",
    "# Filter 2nd innings only\n",
    "df = df[df['innings'] == 2]\n",
    "\n",
    "# Group by match/over\n",
    "df['over'] = df['ball'].astype(str).str.extract(r'^(\\d+)').astype(float)\n",
    "grouped = df.groupby(['match_id', 'over']).agg(\n",
    "    current_score=('total_runs', 'sum'),\n",
    "    wickets=('player_dismissed', lambda x: x.notna().sum())\n",
    ").groupby(level=0).cumsum().reset_index()\n",
    "\n",
    "# Target scores (from 1st innings)\n",
    "targets = df[df['innings'] == 2].groupby('match_id')['total_runs'].sum().groupby(level=0).sum()\n",
    "targets.name = 'target_score'\n",
    "\n",
    "# Merge\n",
    "grouped = grouped.merge(targets, on='match_id', how='left')\n",
    "grouped['overs_remaining'] = 50 - grouped['over']\n",
    "grouped['wickets_in_hand'] = 10 - grouped['wickets']\n",
    "grouped['required_run_rate'] = (grouped['target_score'] - grouped['current_score']) / grouped['overs_remaining'].replace(0, 1)\n",
    "\n",
    "# Final label\n",
    "grouped['match_result'] = (grouped['current_score'] >= grouped['target_score']).astype(int)\n",
    "\n",
    "# Filter valid\n",
    "grouped = grouped.dropna()\n",
    "\n",
    "# Train model\n",
    "features = ['current_score', 'wickets_in_hand', 'overs_remaining', 'required_run_rate']\n",
    "X = grouped[features]\n",
    "y = grouped['match_result']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model, \"win_probability_model.pkl\")\n",
    "print(\"✅ Model trained and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63ab054-ef54-4695-86e1-2281345d596e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be35c5f7-21a4-4f21-aa51-05e2fb447594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1257fb2d-41f3-495f-8816-e41fc74747b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBBoost##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4ac40a1f-d36d-4658-9cf2-f512b6bd0713",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [19:01:18] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ XGBoost Model Trained\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     18396\n",
      "           1       0.93      0.96      0.94       508\n",
      "\n",
      "    accuracy                           1.00     18904\n",
      "   macro avg       0.96      0.98      0.97     18904\n",
      "weighted avg       1.00      1.00      1.00     18904\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "# Load the ODI ball-by-ball dataset\n",
    "df = pd.read_csv(\"ODI_Match_Data.csv\", low_memory=False)\n",
    "\n",
    "# Feature engineering\n",
    "df['total_runs'] = df['runs_off_bat'] + df['extras']\n",
    "df['over'] = df['ball'].astype(str).str.extract(r'^(\\d+)').astype(float)\n",
    "\n",
    "# Use only 2nd innings\n",
    "df = df[df['innings'] == 2]\n",
    "\n",
    "# Group by match and over\n",
    "grouped = df.groupby(['match_id', 'over']).agg(\n",
    "    current_score=('total_runs', 'sum'),\n",
    "    wickets=('player_dismissed', lambda x: x.notna().sum())\n",
    ").groupby(level=0).cumsum().reset_index()\n",
    "\n",
    "# Target scores from 1st innings\n",
    "targets = df.groupby('match_id')['total_runs'].sum().groupby(level=0).sum()\n",
    "targets.name = 'target_score'\n",
    "\n",
    "grouped = grouped.merge(targets, on='match_id', how='left')\n",
    "\n",
    "# Create features\n",
    "grouped['overs_remaining'] = 50 - grouped['over']\n",
    "grouped['wickets_in_hand'] = 10 - grouped['wickets']\n",
    "grouped['required_run_rate'] = (grouped['target_score'] - grouped['current_score']) / grouped['overs_remaining'].replace(0, 1)\n",
    "grouped['match_result'] = (grouped['current_score'] >= grouped['target_score']).astype(int)\n",
    "\n",
    "# Clean and drop NaN\n",
    "grouped = grouped.dropna()\n",
    "\n",
    "# Features and labels\n",
    "features = ['current_score', 'wickets_in_hand', 'overs_remaining', 'required_run_rate']\n",
    "X = grouped[features]\n",
    "y = grouped['match_result']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train XGBoost classifier\n",
    "model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(model, \"win_probability_xgb_model.pkl\")\n",
    "\n",
    "# Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"✅ XGBoost Model Trained\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecce644-a255-4691-b860-74e0857e05ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0fce98-9d73-4f8e-aa4d-4719c929828b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c233a5-908c-400d-ba32-cf85e8c246a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2169c0fd-760e-45ea-8a6d-ee83932e8e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########PRESSURE INDEX #######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2cc702f4-b653-47e2-9f51-d33e17b357f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pressure Index data saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"ODI_Match_Data.csv\", low_memory=False)\n",
    "\n",
    "# Use only second innings\n",
    "df = df[df['innings'] == 2]\n",
    "\n",
    "# Extract over from ball number\n",
    "df['over'] = df['ball'].astype(str).str.extract(r'^(\\d+)').astype(float)\n",
    "df['total_runs'] = df['runs_off_bat'] + df['extras']\n",
    "\n",
    "# Group by match and over to get progressive stats\n",
    "df_overwise = df.groupby(['match_id', 'over']).agg(\n",
    "    runs=('total_runs', 'sum'),\n",
    "    wickets=('player_dismissed', lambda x: x.notna().sum())\n",
    ").groupby('match_id').cumsum().reset_index()\n",
    "\n",
    "# Target from 1st innings\n",
    "targets = df[df['innings'] == 2].groupby('match_id')['total_runs'].sum()\n",
    "df_overwise = df_overwise.merge(targets, on='match_id', how='left')\n",
    "df_overwise['overs_completed'] = df_overwise['over']\n",
    "df_overwise['overs_remaining'] = 50 - df_overwise['over']\n",
    "df_overwise['current_run_rate'] = df_overwise['runs'] / df_overwise['overs_completed'].replace(0, 0.1)\n",
    "df_overwise['required_run_rate'] = (df_overwise['total_runs'] - df_overwise['runs']) / df_overwise['overs_remaining'].replace(0, 0.1)\n",
    "\n",
    "# Pressure Index\n",
    "df_overwise['pressure_index'] = df_overwise['required_run_rate'] / df_overwise['current_run_rate']\n",
    "\n",
    "# Save to CSV\n",
    "df_overwise.to_csv(\"pressure_index_over_time.csv\", index=False)\n",
    "print(\"✅ Pressure Index data saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748d4ae6-48af-4b17-a32b-5ea65a591609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716befc0-15be-46c4-a41c-d99d2e79e8d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6533c74e-9ee4-4ce8-b4fa-23605bc0a60c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab2fa15-af99-465f-a3aa-8d4fc2843135",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### DRS REVIEW #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bdaabf61-0fa8-47c7-8f77-30fd297a704a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DRS commentary lines extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\1976662677.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  drs_df['review_outcome'] = drs_df['lower_comment'].apply(detect_outcome)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the ball-by-ball commentary dataset\n",
    "df = pd.read_excel(\"T20_WC_24_All_Matches_Dataset.xlsx\")\n",
    "\n",
    "# Drop missing commentary\n",
    "df = df[df['commentary'].notna()]\n",
    "\n",
    "# Filter lines mentioning LBW or DRS\n",
    "keywords = ['lbw', 'review', 'umpire', 'drs', 'impact', 'original decision', 'ball tracking', 'review retained', 'review lost']\n",
    "pattern = '|'.join(keywords)\n",
    "df['lower_comment'] = df['commentary'].str.lower()\n",
    "drs_df = df[df['lower_comment'].str.contains(pattern)]\n",
    "\n",
    "# Optional NLP tag: Is overturn?\n",
    "def detect_outcome(text):\n",
    "    if 'overturned' in text or 'changed to out' in text or 'reversed' in text:\n",
    "        return 'Overturned'\n",
    "    elif 'original decision stands' in text or 'umpire\\'s call' in text:\n",
    "        return 'Umpire Call'\n",
    "    elif 'lost review' in text:\n",
    "        return 'Lost Review'\n",
    "    elif 'retained review' in text or 'successful review' in text:\n",
    "        return 'Successful Review'\n",
    "    return 'Unclear'\n",
    "\n",
    "drs_df['review_outcome'] = drs_df['lower_comment'].apply(detect_outcome)\n",
    "\n",
    "# Save it\n",
    "drs_df.to_csv(\"drs_review_events.csv\", index=False)\n",
    "print(\"✅ DRS commentary lines extracted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ee317f-b736-4604-bad2-e185e9096e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a243fc4-9fe8-4cb7-9936-334a9b8a9ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750ddb65-3f33-4e98-a2d6-3738d0f13f3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13456cc-fb1f-4d93-802c-eb443fea1fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# VENUE INSIGHTS #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95165d0-27d6-4aed-8d36-e6bf5daac501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c768d43b-668a-4895-890e-dfa249cdf4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2131266525.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_first['wicket_type'] = df_first['wicket_type'].fillna(\"none\")\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2131266525.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_first['is_spin'] = df_first['wicket_type'].str.contains('|'.join(spin_keywords)).astype(int)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15352\\2131266525.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_first['is_pace'] = df_first['wicket_type'].str.contains('|'.join(pace_keywords)).astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Clustered venue stats saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load ODI data\n",
    "df = pd.read_csv(\"ODI_Match_Data.csv\", low_memory=False)\n",
    "\n",
    "# Total runs per delivery\n",
    "df['total_runs'] = df['runs_off_bat'] + df['extras']\n",
    "\n",
    "# Get only first innings\n",
    "df_first = df[df['innings'] == 1]\n",
    "\n",
    "# Group by venue to get average 1st innings scores\n",
    "venue_stats = df_first.groupby('venue').agg(\n",
    "    avg_score=('total_runs', 'sum'),\n",
    "    deliveries=('ball', 'count')\n",
    ").reset_index()\n",
    "\n",
    "# Add run rate\n",
    "venue_stats['run_rate'] = venue_stats['avg_score'] / (venue_stats['deliveries'] / 6)\n",
    "\n",
    "# Analyze spin vs pace success\n",
    "spin_keywords = ['caught and bowled', 'bowled', 'lbw']\n",
    "pace_keywords = ['caught', 'run out']\n",
    "\n",
    "df_first['wicket_type'] = df_first['wicket_type'].fillna(\"none\")\n",
    "df_first['is_spin'] = df_first['wicket_type'].str.contains('|'.join(spin_keywords)).astype(int)\n",
    "df_first['is_pace'] = df_first['wicket_type'].str.contains('|'.join(pace_keywords)).astype(int)\n",
    "\n",
    "spin_stats = df_first.groupby('venue')[['is_spin', 'is_pace']].sum().reset_index()\n",
    "venue_stats = venue_stats.merge(spin_stats, on='venue', how='left')\n",
    "\n",
    "# Normalize\n",
    "features = ['avg_score', 'run_rate', 'is_spin', 'is_pace']\n",
    "X = venue_stats[features].fillna(0)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# KMeans clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "venue_stats['cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Save\n",
    "venue_stats.to_csv(\"venue_clustered_stats.csv\", index=False)\n",
    "print(\"✅ Clustered venue stats saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd315a0c-ec9b-45b0-b17c-d817f53af353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa879e9-24fa-4dea-9d3e-e7acb3fd0cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e102cd-4e19-4956-9276-173497df9b45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea314a2-6dae-447a-8ec5-65bac0885e2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3c7ff1-2bad-4cb7-bb3c-0653975830d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### MOMENTUM SHIFT ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ff3acac9-1861-4870-9561-baeaab728719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Momentum shifts extracted.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"ODI_Match_Data.csv\", low_memory=False)\n",
    "\n",
    "# Use 2nd innings (chasing phase)\n",
    "df = df[df['innings'] == 2]\n",
    "\n",
    "# Extract over number\n",
    "df['over'] = df['ball'].astype(str).str.extract(r'^(\\d+)').astype(float)\n",
    "df['total_runs'] = df['runs_off_bat'] + df['extras']\n",
    "df['wicket'] = df['player_dismissed'].notna().astype(int)\n",
    "\n",
    "# Group by match & over\n",
    "grouped = df.groupby(['match_id', 'over']).agg(\n",
    "    over_runs=('total_runs', 'sum'),\n",
    "    wickets=('wicket', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Run rate per over\n",
    "grouped['run_rate'] = grouped['over_runs'] / 1.0  # 6 balls = 1 over\n",
    "grouped['rr_delta'] = grouped.groupby('match_id')['run_rate'].diff().fillna(0)\n",
    "grouped['wkt_delta'] = grouped.groupby('match_id')['wickets'].diff().fillna(0)\n",
    "\n",
    "# Label momentum shifts\n",
    "grouped['momentum_shift'] = ((grouped['rr_delta'].abs() >= 3) | (grouped['wickets'] >= 2)).astype(int)\n",
    "\n",
    "# Save output\n",
    "grouped.to_csv(\"momentum_shift_events.csv\", index=False)\n",
    "print(\"✅ Momentum shifts extracted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9eefab-81e6-46f4-b334-59ee8e81e07d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8957b7b0-2e0a-48fd-b0d7-9e7cf4bc54b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65f8608-9201-43fa-87d2-71aecec32835",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6bbd86-f180-4485-be74-8da26aa81a10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a6b890-449b-47ce-b434-4e2bbb7624fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## LLM SUMMARY ###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "318fbbbe-94e3-4c3c-8fd7-c013bf912e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Over-wise data ready for NLP summarization.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load commentary dataset\n",
    "df = pd.read_excel(\"T20_WC_24_All_Matches_Dataset.xlsx\")\n",
    "\n",
    "# Filter key info\n",
    "df = df[['matchID', 'over', 'bowlerName', 'batsmanName', 'runs', 'isWicket', 'commentary']]\n",
    "df = df.dropna(subset=['commentary'])\n",
    "\n",
    "# Aggregate over-wise summaries\n",
    "overwise = df.groupby(['matchID', 'over', 'bowlerName']).agg({\n",
    "    'runs': 'sum',\n",
    "    'isWicket': 'sum',\n",
    "    'commentary': lambda x: ' '.join(x)\n",
    "}).reset_index()\n",
    "\n",
    "overwise.to_csv(\"llm_summary_input.csv\", index=False)\n",
    "print(\"✅ Over-wise data ready for NLP summarization.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "19cc0af4-c0cc-447c-b107-bd0bfb105365",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[101], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load aggregated data\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Load aggregated data\n",
    "df = pd.read_csv(\"llm_summary_input.csv\")\n",
    "\n",
    "# Load summarization pipeline (BART works well)\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "def create_summary(row):\n",
    "    text = f\"Over {row['over']} by {row['bowlerName']}. {row['commentary']}\"\n",
    "    try:\n",
    "        summary = summarizer(text, max_length=50, min_length=15, do_sample=False)[0]['summary_text']\n",
    "    except:\n",
    "        summary = \"⚠️ Unable to generate summary\"\n",
    "    return summary\n",
    "\n",
    "# Apply to each row (limit to top 20 for performance)\n",
    "df['summary'] = df.head(20).apply(create_summary, axis=1)\n",
    "df[['matchID', 'over', 'bowlerName', 'summary']].to_csv(\"llm_generated_summaries.csv\", index=False)\n",
    "\n",
    "print(\"✅ Summaries generated using LLM.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fd050e54-7d11-4eca-991c-0e578313e40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.33.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.2-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Downloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "   ---------------------------------------- 0.0/10.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/10.5 MB 3.6 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.9/10.5 MB 10.9 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.4/10.5 MB 18.8 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 4.0/10.5 MB 22.9 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.5/10.5 MB 25.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.1/10.5 MB 25.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.7/10.5 MB 26.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.1/10.5 MB 28.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.5/10.5 MB 28.5 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.33.1-py3-none-any.whl (515 kB)\n",
      "   ---------------------------------------- 0.0/515.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 515.4/515.4 kB 31.6 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "   ---------------------------------------- 0.0/308.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 308.9/308.9 kB 18.7 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.21.2-cp39-abi3-win_amd64.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ----------------------------- ---------- 1.8/2.5 MB 58.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 32.0 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.33.1 safetensors-0.5.3 tokenizers-0.21.2 transformers-4.52.4\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c562af57-1f46-45de-a676-819704449d64",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:73\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 73\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tensorflow_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[107], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2045\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2043\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   2044\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2045\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m   2046\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   2047\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2075\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2073\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 2075\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2073\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2071\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   2072\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2073\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   2074\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   2075\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdynamic_module_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_class_from_dynamic_module\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedFeatureExtractor\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_processing_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseImageProcessor\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_auto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoConfig\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction_auto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FEATURE_EXTRACTOR_MAPPING, AutoFeatureExtractor\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\image_processing_utils.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_processing_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchFeature, ImageProcessingMixin\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_transforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m center_crop, normalize, rescale\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChannelDimension, get_image_size\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\image_transforms.py:47\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tf_available():\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_flax_available():\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mjnp\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\__init__.py:40\u001b[0m\n\u001b[0;32m     37\u001b[0m _os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:88\u001b[0m\n\u001b[0;32m     86\u001b[0m     sys\u001b[38;5;241m.\u001b[39msetdlopenflags(_default_dlopen_flags)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     89\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;241m.\u001b[39mformat_exc()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     90\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFailed to load the native TensorFlow runtime.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     91\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee https://www.tensorflow.org/install/errors \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     92\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfor some common causes and solutions.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     93\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you need help, create an issue \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     94\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat https://github.com/tensorflow/tensorflow/issues \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     95\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand include the entire stack trace above this error message.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9bcae135-c221-46f5-be02-fffa0643717f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:73\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 73\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tensorflow_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load aggregated data\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2045\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2043\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   2044\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2045\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m   2046\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   2047\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2075\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2073\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 2075\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2073\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2071\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   2072\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2073\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   2074\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   2075\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdynamic_module_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_class_from_dynamic_module\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedFeatureExtractor\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_processing_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseImageProcessor\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_auto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoConfig\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction_auto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FEATURE_EXTRACTOR_MAPPING, AutoFeatureExtractor\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\image_processing_utils.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_processing_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchFeature, ImageProcessingMixin\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_transforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m center_crop, normalize, rescale\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChannelDimension, get_image_size\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\image_transforms.py:47\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tf_available():\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_flax_available():\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mjnp\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\__init__.py:40\u001b[0m\n\u001b[0;32m     37\u001b[0m _os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:88\u001b[0m\n\u001b[0;32m     86\u001b[0m     sys\u001b[38;5;241m.\u001b[39msetdlopenflags(_default_dlopen_flags)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     89\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;241m.\u001b[39mformat_exc()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     90\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFailed to load the native TensorFlow runtime.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     91\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee https://www.tensorflow.org/install/errors \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     92\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfor some common causes and solutions.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     93\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you need help, create an issue \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     94\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat https://github.com/tensorflow/tensorflow/issues \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     95\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand include the entire stack trace above this error message.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Load aggregated data\n",
    "df = pd.read_csv(\"llm_summary_input.csv\")\n",
    "\n",
    "# Load summarization pipeline (BART works well)\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "def create_summary(row):\n",
    "    text = f\"Over {row['over']} by {row['bowlerName']}. {row['commentary']}\"\n",
    "    try:\n",
    "        summary = summarizer(text, max_length=50, min_length=15, do_sample=False)[0]['summary_text']\n",
    "    except:\n",
    "        summary = \"⚠️ Unable to generate summary\"\n",
    "    return summary\n",
    "\n",
    "# Apply to each row (limit to top 20 for performance)\n",
    "df['summary'] = df.head(20).apply(create_summary, axis=1)\n",
    "df[['matchID', 'over', 'bowlerName', 'summary']].to_csv(\"llm_generated_summaries.csv\", index=False)\n",
    "\n",
    "print(\"✅ Summaries generated using LLM.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2c6ee28b-0b00-496e-9aac-c22c01f6d265",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_TrimmedRelease' from 'packaging.version' (C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\packaging\\version.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[109], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\__init__.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config, prefer_gpu, require_cpu, require_gpu  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m util\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\pipeline\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattributeruler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AttributeRuler\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdep_parser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DependencyParser\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01medit_tree_lemmatizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EditTreeLemmatizer\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\pipeline\\attributeruler.py:6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Dict, Iterable, List, Optional, Tuple, Union\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msrsly\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m util\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Errors\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Language\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\util.py:48\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcatalogue\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Registry, RegistryError\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequirements\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Requirement\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecifiers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InvalidSpecifier, SpecifierSet\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InvalidVersion, Version\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\packaging\\requirements.py:8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Iterator\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_parser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_requirement \u001b[38;5;28;01mas\u001b[39;00m _parse_requirement\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tokenizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParserSyntaxError\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmarkers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Marker, _normalize_extra_values\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\packaging\\_parser.py:12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mast\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NamedTuple, Sequence, Tuple, Union\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tokenizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DEFAULT_RULES, Tokenizer\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mNode\u001b[39;00m:\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, value: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\packaging\\_tokenizer.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataclasses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataclass\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Iterator, NoReturn\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecifiers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Specifier\n\u001b[0;32m     11\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mToken\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\packaging\\specifiers.py:18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable, Iterable, Iterator, TypeVar, Union\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m canonicalize_version\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Version\n\u001b[0;32m     21\u001b[0m UnparsedVersion \u001b[38;5;241m=\u001b[39m Union[Version, \u001b[38;5;28mstr\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\packaging\\utils.py:12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NewType, Tuple, Union, cast\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtags\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tag, parse_tag\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InvalidVersion, Version, _TrimmedRelease\n\u001b[0;32m     14\u001b[0m BuildTag \u001b[38;5;241m=\u001b[39m Union[Tuple[()], Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]\n\u001b[0;32m     15\u001b[0m NormalizedName \u001b[38;5;241m=\u001b[39m NewType(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNormalizedName\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_TrimmedRelease' from 'packaging.version' (C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\packaging\\version.py)"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "df = pd.read_csv(\"llm_summary_input.csv\")\n",
    "\n",
    "def rule_based_summary(row):\n",
    "    commentary = row['commentary'].lower()\n",
    "    bowler = row['bowlerName']\n",
    "    runs = row['runs']\n",
    "    wickets = row['isWicket']\n",
    "    \n",
    "    if wickets >= 2:\n",
    "        return f\"{bowler} struck twice in over {row['over']}, a game-changer.\"\n",
    "    elif runs <= 4:\n",
    "        return f\"Tight over by {bowler}, giving just {runs} runs.\"\n",
    "    elif runs >= 15:\n",
    "        return f\"{bowler} leaked {runs} in over {row['over']}, costly phase.\"\n",
    "    else:\n",
    "        return f\"Balanced over from {bowler} with {runs} runs.\"\n",
    "\n",
    "df['summary'] = df.apply(rule_based_summary, axis=1)\n",
    "df[['matchID', 'over', 'bowlerName', 'summary']].to_csv(\"spacy_rule_summaries.csv\", index=False)\n",
    "print(\"✅ spaCy summaries saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bae443be-1d30-4ec5-944d-d4f8d94c8936",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.8.0) was trained with spaCy v3.8.0 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9f269029-835c-454d-849b-ea908a56d276",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:73\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 73\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tensorflow_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[113], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load aggregated data\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2045\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2043\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   2044\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2045\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m   2046\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   2047\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2075\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2073\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 2075\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2073\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2071\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   2072\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2073\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   2074\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   2075\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdynamic_module_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_class_from_dynamic_module\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedFeatureExtractor\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_processing_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseImageProcessor\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_auto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoConfig\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction_auto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FEATURE_EXTRACTOR_MAPPING, AutoFeatureExtractor\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\image_processing_utils.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_processing_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchFeature, ImageProcessingMixin\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_transforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m center_crop, normalize, rescale\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChannelDimension, get_image_size\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\image_transforms.py:47\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tf_available():\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_flax_available():\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mjnp\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\__init__.py:40\u001b[0m\n\u001b[0;32m     37\u001b[0m _os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:88\u001b[0m\n\u001b[0;32m     86\u001b[0m     sys\u001b[38;5;241m.\u001b[39msetdlopenflags(_default_dlopen_flags)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     89\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;241m.\u001b[39mformat_exc()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     90\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFailed to load the native TensorFlow runtime.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     91\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee https://www.tensorflow.org/install/errors \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     92\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfor some common causes and solutions.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     93\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you need help, create an issue \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     94\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat https://github.com/tensorflow/tensorflow/issues \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     95\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand include the entire stack trace above this error message.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Load aggregated data\n",
    "df = pd.read_csv(\"llm_summary_input.csv\")\n",
    "\n",
    "# Load summarization pipeline (BART works well)\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "def create_summary(row):\n",
    "    text = f\"Over {row['over']} by {row['bowlerName']}. {row['commentary']}\"\n",
    "    try:\n",
    "        summary = summarizer(text, max_length=50, min_length=15, do_sample=False)[0]['summary_text']\n",
    "    except:\n",
    "        summary = \"⚠️ Unable to generate summary\"\n",
    "    return summary\n",
    "\n",
    "# Apply to each row (limit to top 20 for performance)\n",
    "df['summary'] = df.head(20).apply(create_summary, axis=1)\n",
    "df[['matchID', 'over', 'bowlerName', 'summary']].to_csv(\"llm_generated_summaries.csv\", index=False)\n",
    "\n",
    "print(\"✅ Summaries generated using LLM.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "48cef020-031f-401a-a430-44ea92cba528",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "Could not import module 'pipeline'. Are this object's requirements defined correctly?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2045\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2044\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2045\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m   2046\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2075\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 2075\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2073\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2072\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2073\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:26\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedFeatureExtractor\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_processing_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseImageProcessor\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_auto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoConfig\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\image_processing_utils.py:22\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_processing_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchFeature, ImageProcessingMixin\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_transforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m center_crop, normalize, rescale\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChannelDimension, get_image_size\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\image_transforms.py:47\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tf_available():\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_flax_available():\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[115], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m      3\u001b[0m summarizer \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummarization\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/bart-large-cnn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m result \u001b[38;5;241m=\u001b[39m summarizer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe bowler delivered a brilliant over conceding only 3 runs and taking a wicket.\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2048\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2046\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   2047\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 2048\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[0;32m   2049\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import module \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Are this object\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms requirements defined correctly?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2050\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   2052\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[0;32m   2053\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: Could not import module 'pipeline'. Are this object's requirements defined correctly?"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "result = summarizer(\"The bowler delivered a brilliant over conceding only 3 runs and taking a wicket.\", max_length=30)\n",
    "print(result[0]['summary_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49f24ca9-958d-46b3-828e-3e87db68ca6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d57370f42b248088283c40fe08d4c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ASUS\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79cdd18d819410b9e3d6c5774570e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f061f625ef549c2a9a983811d60e9d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f57a0f9e50f84924a63c5937c86093dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "064bcdabe8774916a9eff342cd131d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb7d41fad3c49faa548f7a102a1b37f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Your max_length is set to 30, but your input_length is only 20. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Summary: The bowler delivered a brilliant over conceding only 3 runs and taking a wicket.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "result = summarizer(\n",
    "    \"The bowler delivered a brilliant over conceding only 3 runs and taking a wicket.\",\n",
    "    max_length=30,\n",
    "    min_length=10\n",
    ")\n",
    "print(\"✅ Summary:\", result[0]['summary_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abb196aa-16a1-4137-a719-79ab9717994b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Your max_length is set to 40, but your input_length is only 31. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=15)\n",
      "Your max_length is set to 40, but your input_length is only 24. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=12)\n",
      "Your max_length is set to 40, but your input_length is only 30. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=15)\n",
      "Your max_length is set to 40, but your input_length is only 22. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n",
      "Your max_length is set to 40, but your input_length is only 31. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=15)\n",
      "Your max_length is set to 40, but your input_length is only 26. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=13)\n",
      "Your max_length is set to 40, but your input_length is only 34. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=17)\n",
      "Your max_length is set to 40, but your input_length is only 28. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=14)\n",
      "Your max_length is set to 40, but your input_length is only 25. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=12)\n",
      "Your max_length is set to 40, but your input_length is only 39. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n",
      "Your max_length is set to 40, but your input_length is only 22. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM-based summaries saved to 'llm_generated_summaries.csv'\n"
     ]
    }
   ],
   "source": [
    "# 🔧 Install required packages if not already\n",
    "# pip install pandas transformers torch openpyxl\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_excel(\"T20_WC_24_All_Matches_Dataset.xlsx\")\n",
    "\n",
    "# Preprocess: keep key fields and clean\n",
    "df = df[['matchID', 'over', 'bowlerName', 'runs', 'isWicket', 'commentary']]\n",
    "df = df.dropna(subset=['commentary'])\n",
    "\n",
    "# Group by over for summarization\n",
    "agg_df = df.groupby(['matchID', 'over', 'bowlerName']).agg({\n",
    "    'runs': 'sum',\n",
    "    'isWicket': 'sum',\n",
    "    'commentary': lambda x: ' '.join(x)\n",
    "}).reset_index()\n",
    "\n",
    "# Load transformer summarizer (BART)\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Generate summaries (limit for speed, or loop all)\n",
    "def generate_summary(row):\n",
    "    text = f\"Over {int(row['over'])} by {row['bowlerName']}. {row['commentary']}\"\n",
    "    try:\n",
    "        result = summarizer(text, max_length=40, min_length=15, do_sample=False)\n",
    "        return result[0]['summary_text']\n",
    "    except:\n",
    "        return \"⚠️ Summary failed.\"\n",
    "\n",
    "agg_df['summary'] = agg_df.head(20).apply(generate_summary, axis=1)  # limit rows if needed\n",
    "agg_df.to_csv(\"llm_generated_summaries.csv\", index=False)\n",
    "\n",
    "print(\"✅ LLM-based summaries saved to 'llm_generated_summaries.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dccb17b5-f875-43ba-8602-1459c73e58bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Your max_length is set to 40, but your input_length is only 34. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=17)\n",
      "Your max_length is set to 40, but your input_length is only 27. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=13)\n",
      "Your max_length is set to 40, but your input_length is only 33. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=16)\n",
      "Your max_length is set to 40, but your input_length is only 27. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=13)\n",
      "Your max_length is set to 40, but your input_length is only 36. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=18)\n",
      "Your max_length is set to 40, but your input_length is only 33. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=16)\n",
      "Your max_length is set to 40, but your input_length is only 38. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n",
      "Your max_length is set to 40, but your input_length is only 35. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=17)\n",
      "Your max_length is set to 40, but your input_length is only 29. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=14)\n",
      "Your max_length is set to 40, but your input_length is only 25. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=12)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: llm_commentary_enhanced.csv with batsman, sentiment, impact.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load and clean data\n",
    "df = pd.read_excel(\"T20_WC_24_All_Matches_Dataset.xlsx\")\n",
    "df = df[['matchID', 'over', 'bowlerName', 'batsmanName', 'isWicket', 'runs', 'commentary']]\n",
    "df = df.dropna(subset=['commentary'])\n",
    "\n",
    "# Aggregate per over\n",
    "grouped = df.groupby(['matchID', 'over', 'bowlerName']).agg({\n",
    "    'runs': 'sum',\n",
    "    'isWicket': 'sum',\n",
    "    'batsmanName': lambda x: ', '.join(set(x)),\n",
    "    'commentary': lambda x: ' '.join(x)\n",
    "}).reset_index()\n",
    "\n",
    "# Load summarizer\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Define sentiment tagger (basic rule-based)\n",
    "def get_sentiment(text):\n",
    "    pos_keywords = [\"great\", \"brilliant\", \"excellent\", \"dot ball\", \"wicket\", \"tight\"]\n",
    "    neg_keywords = [\"poor\", \"expensive\", \"wide\", \"no ball\", \"bad\", \"costly\"]\n",
    "    text = text.lower()\n",
    "    if any(word in text for word in pos_keywords):\n",
    "        return \"Positive\"\n",
    "    elif any(word in text for word in neg_keywords):\n",
    "        return \"Negative\"\n",
    "    return \"Neutral\"\n",
    "\n",
    "# Define impact rating\n",
    "def impact(row):\n",
    "    if row['isWicket'] >= 2 or row['runs'] <= 4:\n",
    "        return \"High\"\n",
    "    elif row['isWicket'] == 1 or row['runs'] <= 8:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Low\"\n",
    "\n",
    "# Generate summaries\n",
    "def summarize_row(row):\n",
    "    text = f\"Over {int(row['over'])} by {row['bowlerName']} to {row['batsmanName']}. {row['commentary']}\"\n",
    "    try:\n",
    "        summary = summarizer(text, max_length=40, min_length=15, do_sample=False)\n",
    "        return summary[0]['summary_text']\n",
    "    except:\n",
    "        return \"Summary failed\"\n",
    "\n",
    "grouped['summary'] = grouped.head(20).apply(summarize_row, axis=1)\n",
    "grouped['sentiment'] = grouped['commentary'].apply(get_sentiment)\n",
    "grouped['impact'] = grouped.apply(impact, axis=1)\n",
    "\n",
    "# Save final dataset\n",
    "grouped.to_csv(\"llm_commentary_enhanced.csv\", index=False)\n",
    "print(\"✅ Saved: llm_commentary_enhanced.csv with batsman, sentiment, impact.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fadaa30-f339-4285-9fd7-9a79863b7a96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
